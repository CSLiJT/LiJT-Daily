---
title: 变分自编码器（VAE）的本质：使用神经网络实现变分推断的计算
mathjax: true
codeblock:
  enable: true
  show_result: true
date: 2023-03-20 08:10:36
tags: 深度学习 人工智能
categories: 深度学习
---

甲方的增量需求，以及神经网络上的约束条件，都可以转化成损失函数的惩罚项。———— 沃·兹基硕德

变分自编码器是一种经典的深度生成模型，在图像生成、协同过滤等任务上有着重要的应用，有着举足轻重的历史地位。

<!--more-->
## 回顾：变分推断的本质

书接上文。在变分推断的介绍中，我们已经明确了变分推断方法的本质：用一个简单分布拟合另一个复杂分布。并且我们已知，变分推断包含一下几个步骤：
1. 给定观测数据$X$，计算隐变量Z的简单分布$q_\phi(Z)$. 其中$\phi$为分布族$q(\cdot)$的参数，例如正态分布中的均值和方差；二项分布中的正面概率$p$.
2. 根据观测数据$X$，计算证据下界$ELBO(q)=E_Z[\log p(X|Z)]-KL(q_\phi(Z)||p(Z))$.
3. 优化简单分布族的参数$\phi$，使得证据下界最大化. 这一步蕴含两个要素：
   1. 最大化**观测变量**$X$的拟合值（最优重建）
   2. 使隐变量估计简单分布与其先验分布尽可能地接近

## 正题：变分自编码器的本质
**变分自编码器的本质在于，使用神经网络建模隐变量的简单分布$q_\phi(\cdot)$ 和观测变量的后验概率 $p(X|Z)$ ，使用反向传播方法完成证据下界最大化的参数优化过程。**

我们首先给出变分自编码器的结构图，然后介绍它具体的工作原理。

![图中展示的是<简单分布族=高斯分布>的变分自编码器.Encoder和Decoder为神经网络. $P_\phi(\cdot)$等价于本文中的$q_\phi(\cdot)$](https://pic4.zhimg.com/80/v2-448d96d1d9d77ca2a01a7d85447782c7.jpg)

### 变分自编码器的工作流程

变分自编码器的工作流程如下：
1. 输入观测数据$X$，使用基于神经网络的Encoder计算隐变量$Z$的分布参数（注意，计算结果不是隐变量本身，因为变分推断中隐变量是随机变量，需要从分布中采样得到）
2. 使用重参数化技巧采样得到隐变量$Z$（例如上图中的$Z=\mu+\sigma\odot\epsilon$得到隐变量$Z$）.**重参数化技巧的内涵在于将随机变量的随机性与其分布特征解耦，从而能够实现后者的反向传播优化**.
3. 输入隐变量$Z$，使用基于神经网络的Decoder计算重建的观测数据$\hat{X}$
4. 计算损失函数（即证据下界，$ELBO$），使用反向传播方法优化Encoder和Decoder.

### ELBO的本质：带惩罚项的损失函数
在上文中，我们已经说明了，证据下界$ELBO$的定义如下：

$$
ELBO(q)=E_Z[\log p(X|Z)]-KL(q_\phi(Z)||p(Z))
$$

本质上，等式右侧的第一项，是用于**最优化重建观测变量$\hat{X}$**的损失函数主项，可以用交叉熵、均方根误差等常见的神经网络损失函数代替；第二项是用于**使隐变量$Z$的后验分布参数尽可能接近先验分布** 的惩罚项，例如先验分布为$N(0,1)$，则该项使得Encoder生成的$Z$的期望尽可能接近0，方差尽可能接近1. 一个经典的ELBO的实现如下：

$$
L(\Omega)=CrossEntropy(X,\hat{X})−1/2(log\sigma_\phi^2−\sigma_\phi^2−\mu_\phi^2+1)
$$

其中$\Omega$是变分自编码器的所有可优化参数，$\mu_\phi$和$\sigma_\phi^2$分别是隐变量$Z$的期望和方差. 该损失函数适用于**标准正态先验分布**.

另一个适用于**标准二项先验分布**的ELBO实现如下：

$$
L(\Omega)=CrossEntropy(X,\hat{X})−(p_\phi\log p_\phi + (1-p_\phi)\log(1-p_\phi))
$$

其中$p_\phi$是由Encoder计算的$Z=1$的概率. $Z$服从二项分布.

## 参考文献
[1] [JKRD.变分推断的本质](https://cslijt.github.io/LiJT-Daily/2022/08/19/2022-08-19-vi/)

[2] David M. Blei, Alp Kucukelbir, & Jon D. McAuliffe (2016). Variational Inference: A Review for Statisticians. CoRR, abs/1601.00670.

[3] DOERSCH C. Tutorial on Variational Autoencoders[J]. stat, 2016, 1050: 13.

<section class="post-full-comments">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
    <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
    <div id="gitalk-container"></div>
    <script>
        var gitalk = new Gitalk({
            clientID: 'e1bbf465a324641f76ce',
            clientSecret: 'b865ad952a6494eb48283884abbe479d3f89f4a4',
            repo: 'LiJT-Daily-Comments',
            owner: 'CSLiJT',
            admin: ['CSLiJT'], //这里可以填写具有写权限的用户名列表，用来初始化Issues的
            id: decodeURI(window.location.pathname),
            distractionFreeMode: true // Facebook-like distraction free mode
        });
        gitalk.render('gitalk-container');
    </script>
</section>