---
title: 变分推断学习笔记.1
mathjax: true
codeblock:
  enable: true
  show_result: true
date: 2022-08-19 16:13:47
tags:
categories:
---
变分推断的基本概念、证据下界（ELBO）。
<!--more-->

### 背景
- 变分推断（Variational Inference, VI）主要用于解决大数据场景下的隐变量后验分布估计问题（在数据量较小时，可以使用MCMC方法）。给定一个数据集中的观测值$X$和隐变量$Z$，隐变量$Z$会影响观测值$X$的取值，即$Z\to X$。将$Z$和$X$当作随机变量，我们希望求得对$Z$的后验分布$p(Z|X)$的估计，记作$q(Z)$。
- 变分推断的本质是选取一个恰当的分布族$\mathcal{L}$，从该分布族中选取一个最好的$q(Z)$，使得$q(Z)$与$p(Z|X)$尽可能接近。

### 两个分布函数的距离度量
- VI中使用KL散度（Kullback & Leibler divergence）度量$q(Z)$与$p(Z|X)$的相似性，即

$$
KL(q(Z)||p(Z|X)) = E_Z\left[\log\frac{q(Z)}{p(Z|X)}\right] 
$$

- 那么最优的后验分布估计为：
  
$$
q^*(Z) = {\arg\min}_{q(Z)\in\mathcal{L}}KL(q(Z)||p(Z|X))
$$

#### 证据下界（ELBO）的引入
- 然而上式中的$p(Z|X)$是难以计算的：
  
$$
p(Z|X) = \frac{p(Z,X)}{p(X)}=\frac{p(Z,X)}{\int_{-\infty}^{+\infty}p(X|Z)p(Z)dZ}
$$

- 计算难点主要在于观测变量的边缘分布$p(X)$（也被称作**证据(evidence)**）。如果隐变量维度很高，那么计算开销将非常大。为此，需要在$KL(q(Z)||p(Z|X))$动一些手脚：
  
$$
KL(q(Z)||p(Z|X)) = E_Z\left[\log\frac{q(Z)}{p(Z|X)}\right]\\
= E_Z[\log q(Z)] - E_Z[\log p(Z|X)]\\
= E_Z[\log q(Z)] - E_Z[\log p(Z,X)] + E_Z[\log p(X)] \\
= - ELBO(q) + E_Z[\log p(X)]
$$

- 由上面的变换可知，由于$KL(q(Z)||p(Z|X))$取期望的对象是$Z$，这对于证据$p(X)$是没有关系的！因此我们只需要计算式中$ELBO(q)$，将其最大化，即能最小化$KL(q(Z)||p(Z|X))$：
  
$$
ELBO(q) = E_Z[\log p(Z,X)] - E_Z[\log q(Z)]
$$
$$
q^*(Z) = {\arg\max}_{q(Z)\in\mathcal{L}}ELBO(q)
$$

#### ELBO的性质
- 我们对ELBO做一些变换：
$$
ELBO(q) = E_Z[\log p(Z,X)] - E_Z[\log q(Z)] \\
= E_Z[\log p(X|Z)] + E_Z[\log p(Z)] - E_Z[\log q(Z)] \\
= E_Z[\log p(X|Z)] - KL(q(Z)||p(Z))
$$
- 可见ELBO由**观测变量的后验分布**和**隐变量估计分布与其先验分布的KL散度**两部分组成（注意KL散度是非对称的）。因此最大化$ELBO(q)$相当于同时做以下两件事：
  1. 最大化**观测变量**的后验分布对数期望
  2. 使隐变量估计分布与其先验分布尽量接近
  - 是不是有点贝叶斯推断的感觉了？
- ELBO 的另一个性质来源于它的名字，即证据（的）下界：
$$
 \log p(X) = E_Z[\log p(X)] \\
 = KL(q(Z)||p(Z|X)) + ELBO(q) \\
 \geq ELBO(q)
$$
- 关键点在于$KL(\cdot)\geq 0$。VI的目标是最大化ELBO，而ELBO最大不会超过$\log p(X)$。个人认为这一结论说明，观测数据本身质量好坏决定了模型的拟合效果。因此数据在VI（乃至机器学习）中扮演极端重要的角色。

### 补充：$KL(\cdot)\geq 0$ 的证明
- 对于任意的两个概率密度函数$p(x)$和$q(x)$，
  $$
KL(q(x)||p(x)) = E_{q(x)}\left[\log \frac{q(x)}{p(x)}\right]
  $$
- 证明的核心在于对log函数（凹函数）使用Jensen不等式。证明如下：
$$
 E_{q(x)}\left[\log \frac{q(x)}{p(x)}\right] = - E_{q(x)}\left[\log \frac{p(x)}{q(x)}\right]
$$
$$
\geq -\log E_{q(x)}\left[\frac{p(x)}{q(x)}\right]\quad\text{(Jensen 不等式)}
$$
$$
= -\log\int_{-\infty}^{+\infty}\frac{p(x)}{q(x)}\cdot q(x) dx
$$
$$
= -\log\int_{-\infty}^{+\infty}\frac{p(x)}{q(x)}\cdot q(x) dx
$$
$$
= -\log\int_{-\infty}^{+\infty}p(x)dx
$$
$$
= 0  \quad\text{(概率密度函数的归一化性质)}
$$

### 参考文献
[1] David M. Blei, Alp Kucukelbir, & Jon D. McAuliffe (2016). Variational Inference: A Review for Statisticians. CoRR, abs/1601.00670.


  
<section class="post-full-comments">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
    <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
    <div id="gitalk-container"></div>
    <script>
        var gitalk = new Gitalk({
            clientID: 'e1bbf465a324641f76ce',
            clientSecret: 'b865ad952a6494eb48283884abbe479d3f89f4a4',
            repo: 'LiJT-Daily-Comments',
            owner: 'CSLiJT',
            admin: ['CSLiJT'], //这里可以填写具有写权限的用户名列表，用来初始化Issues的
            id: document.title+document.date,
            distractionFreeMode: false // Facebook-like distraction free mode
        });
        gitalk.render('gitalk-container');
    </script>
</section>