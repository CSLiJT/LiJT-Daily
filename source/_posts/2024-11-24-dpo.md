---
title: 学习笔记：直接偏好优化（Direct Preference Optimization）原理
mathjax: true
codeblock:
  enable: true
  show_result: true
date: 2024-11-24 16:24:53
tags: 深度学习 LLM
categories: 学习笔记
---

DPO是一种语言模型（language model）的后训练算法，无需使用RL而使之输出与人类偏好对齐。

<!--more-->

## 算法由来
- DPO是一种用于语言模型（language model, LM）的后训练算法。其目标是使得LM的文本输出能够对齐人类偏好（如表述风格、格式等），而无需使用奖励模型（reward model）或其他强化学习算法。
- **DPO核心思想**：偏好优化问题可以转化为二分类问题。

## 算法基本原理
- DPO不需要训练奖励模型。相反，其需要已经打分标注好的偏好优化数据集。具体而言，偏好优化数据集表示为为$D=\{(x,y_a,y_r)|x\in X, y_a\in Y, y_r\in Y\}$，即每条数据由三个项构成。其中$x$为prompt，也就是给LM的输入。$y_a$和$y_r$分别是对于$x$的两种回答。其中$y_a$更为人类所偏好；$y_r$更为人类所不偏好
- DPO的核心公式——Bradley-Terry模型：
$$
P(y_a > y_r|x) = \frac{\zeta(y_a|x)}{\zeta(y_a|x) + \zeta(y_r|x)}
$$
$$
= \frac{e^{r(x,y_a)}}{e^{r(x,y_a)}+e^{r(x,y_r)}} = \frac{1}{1+e^{-(r(x,y_a) - r(x,y_r))}}
$$
$$
= \sigma(r(x,y_a) - r(x,y_r))
$$
其中$\zeta(\cdot)$是一个实的、正的、处处可导的函数，可以理解为得分函数，为Bradley-Terry模型所定义。在DPO中，要求$\zeta(y|x)=e^{r(x,y)}$，其中$r(x,y)$表示输出$y$相对于输入$x$的奖励得分。这样做的好处是，目标函数可以转化为易于计算的sigmoid函数，即$\sigma(\cdot)$。

- 在DPO中，$r(x,y)$被规定为:
$$
r(x,y) = \beta\log\frac{\pi_{\theta}(y|x)}{\pi_{\text{ref}}(y|x)} = \beta\log\pi_{\theta}(y|x) - \beta\log\pi_{\text{ref}}(y|x)
$$
其中$\beta$是温度参数，控制概率分布的平滑程度。$\pi_{\theta}(y|x)$和$\pi_{\text{ref}}(y|x)$分别是由当前模型$\pi_{\theta}$和参考模型$\pi_{\text{ref}}$给定x时输出y的概率。$\pi_{\theta}$即是我们需要优化的模型，一般初始化为$\pi_{\text{ref}}$。$\pi_{\text{ref}}(y|x)$为常数，在训练开始前可以缓存起来。同时，在使用链式法则计算损失函数中，这一常数会在$\nabla_\theta r(x,y)$中消失。

- 由此，DPO的损失函数可以定义为B-T模型的负对数似然，即：
$$
L_{\text{DPO}}(\pi_{\theta};\pi_{\text{ref}}) = E_{(x,y_a,y_r)\sim D}\left[-\log \left(\sigma\left( \beta\log\frac{\pi_{\theta}(y_a|x)}{\pi_{\text{ref}}(y_a|x)} -  \beta\log\frac{\pi_{\theta}(y_r|x)}{\pi_{\text{ref}}(y_r|x)}\right)\right)\right]
$$
- DPO由于符合Bradley-Terry模型，因此也保留了B-T的一些优良的数学性质，这里不再赘述，有兴趣可读[原始论文](https://arxiv.org/pdf/2305.18290)的Section 5.

## 算法流程
**Step 1. 标注数据** 采样有监督微调（sft）的模型$\pi^{\text{SFT}}$对于每个prompt $x$的两个输出，即$y_1, y_2\sim\pi^{\text{SFT}}(\cdot|x)$。然后人工标注对于两个输出的偏好，形成人类偏好数据集$D = \{(x,y_a,y_r)|x\in X, y_a\in Y, y_r\in Y\}$。

**Step 2. 参考模型初始化** 如果$\pi^{\text{SFT}}$已知，则令参考模型$\pi_{\text{ref}}\leftarrow\pi^{\text{SFT}}$；否则对$\pi^{\text{ref}}$做sft，即$\pi^{\text{ref}} = \arg\max_{\pi} E_{(x,y_a)\in D}[\log \pi(y_a|x)]$。

**Step 3. 模型优化** 首先将语言模型$\pi_{\theta}$初始化为$\pi_{\text{ref}}$。然后将数据集$D$的prompt输入$\pi_{\theta}$和$\pi_{\text{ref}}$；计算损失函数$L_{\text{DPO}}(\pi_{\theta};\pi_{\text{ref}})$；使用梯度下降法最小化损失函数，从而优化语言模型$\pi_{\theta}$。

注：[原始论文](https://arxiv.org/pdf/2305.18290)算法流程的符号略微混乱。原论文的标注数据阶段声称由$\pi_{\text{ref}}$完成，但下文又称偏好数据集通常由$\pi^{\text{SFT}}$生成。此处为避免混淆，按照$\pi^{\text{SFT}}\to\pi_{\text{ref}}\to\pi_{\theta}$的顺序介绍算法流程。

## 参考文献
- Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, & Chelsea Finn (2023). Direct Preference Optimization: Your Language Model is Secretly a Reward Model. In Thirty-seventh Conference on Neural Information Processing Systems.
- 维基百科. Bradley-Terry Model. [链接](https://en.wikipedia.org/wiki/Bradley%E2%80%93Terry_model)
- csdn. 【基础知识】DPO(Direct Preference Optimization)的原理以及公式是怎样的？. [链接](https://blog.csdn.net/u014386899/article/details/136633074)

<section class="post-full-comments">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
    <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
    <div id="gitalk-container"></div>
    <script>
        var gitalk = new Gitalk({
            clientID: 'e1bbf465a324641f76ce',
            clientSecret: 'b865ad952a6494eb48283884abbe479d3f89f4a4',
            repo: 'LiJT-Daily-Comments',
            owner: 'CSLiJT',
            admin: ['CSLiJT'], //这里可以填写具有写权限的用户名列表，用来初始化Issues的
            id: decodeURI(window.location.pathname),
            distractionFreeMode: true // Facebook-like distraction free mode
        });
        gitalk.render('gitalk-container');
    </script>
</section>