---
title: PyTorch踩坑记录（持续更新）
author: LiJT
date: 2021-09-27
tags: 
  - pytorch 
  - 深度学习 
  - coding 
  - 持续更新
key: pytorch20210927
---

本篇记录使用pytorch过程中踩到的各种坑！
<!--more-->

## 声明损失函数时忘记加括号
```python
loss_fn = nn.BCELoss # 应该是 nn.BCELoss()

>> RuntimeError: bool value of Tensor with more than one value is ambiguous
```

## 通过索引赋值后，梯度还能正常反向传播吗？
- 答案：__能__.

### 验证代码

```python
import torch 

def main():
    # x 是输入张量，可求梯度
    x = torch.rand(4)
    x.requires_grad_(True)
    # cache 是中间张量，将x赋给cache
    cache = torch.zeros(3,4)
    optim = torch.optim.Adam([x],lr=1e-2)
    for i in range(10):
        # 按索引赋值
        cache[1,:] = x 
        result = cache * 2
        result = result.view(-1)
        # 求损失函数
        sum = torch.sum(result, dim = 0)
        print(x.data)
        optim.zero_grad()
        sum.backward(retain_graph = True)
        optim.step()
    
    return 

if __name__ == '__main__':
    main()
```

### 验证输出
```
tensor([0.3073, 0.0680, 0.3627, 0.0659])
tensor([0.2973, 0.0580, 0.3527, 0.0559])
tensor([0.2873, 0.0480, 0.3427, 0.0459])
tensor([0.2773, 0.0380, 0.3327, 0.0359])
tensor([0.2673, 0.0280, 0.3227, 0.0259])
tensor([0.2573, 0.0180, 0.3127, 0.0159])
tensor([0.2473, 0.0080, 0.3027, 0.0059])
tensor([ 0.2373, -0.0020,  0.2927, -0.0041])
tensor([ 0.2273, -0.0120,  0.2827, -0.0141])
tensor([ 0.2173, -0.0220,  0.2727, -0.0241])
```
经过 x -> cache -> sum 的计算并反向传播后，可见x的值如上所示有所变化，因此索引将前向传播中间结果赋给Tensor，再在Tensor上做后续运算，能够实现到达输入张量的反向传播！


