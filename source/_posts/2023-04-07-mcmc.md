---
title: MCMC的本质：使用马氏链建模复杂后验分布
mathjax: true
codeblock:
  enable: true
  show_result: true
date: 2023-04-07 11:03:53
tags: 学习笔记 贝叶斯推断
categories: 学习笔记
---

MCMC（蒙特卡洛马尔可夫链）与VI（变分推断），可谓是贝叶斯推断之双雄。 —— 沃·兹基硕德

<!--more-->

## 背景
### MCMC的提出动机
- MCMC的提出，本质上是为了解决贝叶斯推断中，**复杂的后验分布难以建模的问题**。对于这一难题，变分推断的解决方案是，使用一个简单的分布族（如高斯分布）拟合该复杂分布。而MCMC的解决方案是，使用马尔科夫链去逼近这一真实的后验分布，并使用蒙特卡洛采样法从这一复杂分布中采样。MCMC本身是一个算法框架，并不拘泥于算法的实现方式。在MCMC方法中，代表算法实例包括[Metropolis-Hasting (MH)算法](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm)和[Hamiltonian Monte Carlo (HMC)](https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo)算法。

### 回顾：贝叶斯推断
- 无论是MCMC还是VI，其基本的理论框架均是贝叶斯推断，即研究如何从**隐变量的先验分布**和**观测变量关于隐变量的似然分布**，推断出**隐变量的后验分布**的问题。具体地，给定观测变量$D$和隐变量$\theta$，贝叶斯推断基于如下的贝叶斯公式：
$$
P(\theta|D) = \frac{P(D|\theta)P(\theta)}{P(D)}= \frac{P(D|\theta)P(\theta)}{\int_{-\infty}^{+\infty}P(D|\theta')P(\theta')d\theta'},
$$
其中$P(\theta)$被称作**先验分布，通常是人为设定的**，$P(D|\theta)$被称作**似然，是提前定义好的**（就是极大似然估计（MLE）中的“似然”），$P(\theta|D)$被称作**后验分布，是需要我们推断的**，$P(D)$被称作**证据，是与隐变量无关并且难以测量的**（就是变分推断中证据下界的“证据”）。

## MCMC的本质：使用马氏链建模复杂后验分布
### 马尔科夫链的优良性质
- MCMC的成功，依赖于马尔科夫链的**无记忆性**、**平稳性**和**细致平衡条件**。下面介绍这三个性质，并说明为什么MCMC能够有效建模隐变量的复杂后验分布：
1. **无记忆性**：在给定当前状态下，未来的状态只与当前状态有关。
   - 例如，假设一个由随机变量组成的链为$\theta_1\to\theta_2\to\cdots\to\theta_T$，其中$\theta_t,1\leq t\leq T$为该变量在时刻$t$的状态。则无记忆性可表示为：
$$ 
P(\theta_t|\theta_{t-1},\ldots,\theta_1)=P(\theta_t|\theta_{t-1})
$$
1. **平稳性(stationarity)**：对于一个马尔科夫链，如果它满足某个条件，那么在经过充分长的时间后，它会达到一个平稳分布，即使初始状态不同，也最终会收敛到这个分布。
   - 这一性质蕴含着，变量可以从一个随机初始状态出发，收敛到目标分布！
2. **细节平衡条件(detailed balance sheet condition)**：为了保证MCMC算法的正确性和有效性，需要满足细节平衡条件。即，当马尔科夫链满足平稳分布时，对于任意两个状态之间的转移，从一个状态到另一个状态的概率与从另一个状态到该状态的概率之比应该等于两个状态在平稳分布上的概率之比。
   - 具体地，假设随机变量$\theta$的平稳分布为$p(\theta)=\pi(\theta)$，马氏链的状态转移概率为$A$，则细致平衡条件可以表示为：
$$
\pi(\theta_a)\cdot A(\theta_a\to \theta_b) = \pi(\theta_b)\cdot A(\theta_b\to \theta_a)
$$

给定马尔科夫链的上述性质，MCMC的本质可以概括如下：

> **利用马尔科夫链的平稳分布性质，可以通过构建一个符合平稳分布的马尔科夫链来生成服从该分布的样本。具体而言，MCMC方法采用一个迭代算法，在每一步中，根据当前隐变量状态利用转移概率生成一个新的隐变量状态，并以一定的概率接受这个新状态。反复进行这个过程，得到的一系列状态就是我们所需要的隐变量的样本。**

### MCMC的工作流程

1. 确定模型：需要建立一个概率模型，描述数据和未知参数之间的关系，即设计**似然函数**。
2. 设计先验分布：为未知参数设定先验分布，反映对其可能取值的先验认识。尽量选择[**共轭先验分布**](https://zhuanlan.zhihu.com/p/401626004)，能够方便地进行后验推导（部分情况下，共轭先验分布难以选取，因此推导后验十分困难，这也是为什么需要MCMC）。
3. 构建目标函数：根据贝叶斯公式，可以得到后验分布的形式（含分母，即证据$P(D)$）。为了使用MCMC方法，需要构建一个与后验分布成正比例的目标函数。
4. 初始化状态：选择一个初始状态，即一组未知参数的值，并将其设置为马尔科夫链的起点。
5. 进行迭代采样：采用MCMC算法，反复进行状态转移，在每一步中生成新的状态并接受或拒绝该状态。通过多次迭代，得到一串与后验分布成正比的随机样本。
6. 分析结果：根据采样的样本，可以计算后验分布的各种统计量，如均值、方差、置信区间等。也可以通过可视化方法来呈现后验分布的特征，如直方图、密度图等。

需要注意的是，MCMC方法的性能受到多个因素的影响，如初始状态的选择、采样步长的调整、马尔科夫链的收敛性等。因此，在实际应用中需要进行仔细的调参和诊断，以确保结果的可靠性和有效性。

## MCMC的实例：Metropolis-Hasting算法
- 判断一个算法是否是MCMC，其关键在于这一算法的正确性是否依赖于马氏链的**细节平衡条件**。其中，Metropolis-Hasting算法就是一个典型案例。

## 参考文献
1. https://towardsdatascience.com/monte-carlo-markov-chain-mcmc-explained-94e3a6c8de11
2. https://zhuanlan.zhihu.com/p/411689417
3. https://www.cnblogs.com/szqfreiburger/p/11828517.html
4. https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo
5. https://en.wikipedia.org/wiki/Markov_chain_Monte_Carlo
6. https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm
7. https://zhuanlan.zhihu.com/p/401626004
8. https://mc-stan.org/users/documentation/

<section class="post-full-comments">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
    <script src="https://cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
    <div id="gitalk-container"></div>
    <script>
        var gitalk = new Gitalk({
            clientID: 'e1bbf465a324641f76ce',
            clientSecret: 'b865ad952a6494eb48283884abbe479d3f89f4a4',
            repo: 'LiJT-Daily-Comments',
            owner: 'CSLiJT',
            admin: ['CSLiJT'], //这里可以填写具有写权限的用户名列表，用来初始化Issues的
            id: decodeURI(window.location.pathname),
            distractionFreeMode: true // Facebook-like distraction free mode
        });
        gitalk.render('gitalk-container');
    </script>
</section>