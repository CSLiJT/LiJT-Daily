---
title: 'PyTorch: CrossEntropyLoss vs. NLLLoss vs. BCELoss'
author: LiJT
show_author_profile: true
tags: pytorch 深度学习 coding
article_header:
  type: cover
key: pytorch20211007
---

CrossEntropyLoss, NLLLoss 和 BCELoss 本质上都是基于交叉熵(cross entropy)的分类器的损失函数。但是三个函数的输入格式、计算方法和性能（收敛速度）有很大差别。本文记录笔者对此三者的学习笔记和理解。
{:.info}

## 交叉熵(Cross Entropy)

交叉熵(Cross Entropy)是Shannon信息论中一个重要概念，主要用于度量两个概率分布间的差异性信息。或曰，概率分布 $p$ 和概率分布 $q$ 的相似程度。如果 $p$ 和 $q$ 越相似，那么越能用 $p$ 近似表示 $q$ 或用 $q$ 近似表示 $p$ 。定义交叉熵为：

$$
H(p,q) = \sum_x p(x)\log \left(\frac{1}{q(x)}\right)
$$

注意交叉熵不满足对称性。

## nn.CrossEntropyLoss

```python
torch.nn.CrossEntropyLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')
```

### 计算方法

根据[pytorch官方文档](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)，CrossEntropyLoss的输入值为"unnormalized scores for each class"，即未限制在(0,1)上的各个类别的得分。其表达式为

$$
loss(x,class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])} \right)
$$

其中 $x$ 为一个样本，$class$ 为一个类别, $x[j]\in(-\infty,\infty)$ 为分类器给样本 $x$ 在类别 $j$ 上赋予的得分，或当weight不为空时，

$$
loss(x,class) = -weight[class]\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])} \right)
$$

其中 $weight[class]$ 为类别 $class$ 的权重。其值越大，总损失中 $class$ 类所占有的损失项的比重越大。

最终，总损失为每个样本$x$上的损失的加权平均，即

$$
\mathcal{L}(X,class) = \frac{\sum_{x\in X}loss(x,class[x])}{\sum_{x\in X}weight[class[x]]}
$$

### 总结
- 是否支持多类别分类：支持
- 输入得分值域：$(-\infty,\infty)$
- 神经网络输出层是否需要激活/归一化: 不用

## nn.NLLLoss

```python
torch.nn.NLLLoss(weight=None, size_average=None, ignore_index=-100, reduce=None, reduction='mean')
```

### 计算方法

`NLLLoss`的用法，实际在`CrossEntropyLoss`的文档中给出：

This criterion combines **LogSoftmax** and **NLLLoss** in one single class.
{:.success}

也就是说，`CrossEntropyLoss`是`NLLLoss`和`LogSoftmax`的结合体。为看清这一点，我们回到式(1)。其中 $\log\frac{\exp(\cdot)}{\sum_j \exp(\cdot)}$ 就是LogSoftmax.

因此，如果`CrossEntropyLoss`的输入值是 $(x,class)$， 那么 `NLLLoss` 的输入值就是 $LogSoftmax(x), class$。其中 $LogSoftmax(\cdot)$ 需要对 $x$ 的每个分量计算一次，最终 $dim(LogSoftmax(x)) = dim(x)$。除此之外，`NLLLoss` 在其他部分的计算过程与 `CrossEntropyLoss` 完全一致。



<!--more-->